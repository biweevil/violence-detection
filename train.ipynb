{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sorted_containers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-db3e942a81bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msorted_containers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m# os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml --checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ../dataset/NonViolence/NV_1.mp4 --outdir ../dataset --sp')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sorted_containers'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from extract import extract_dataset_archive\n",
    "from split import convert_train_set_to_images\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml --checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ../dataset/NonViolence/NV_1.mp4 --outdir ../dataset --sp')\n",
    "\n",
    "def prepare_dataset():\n",
    "    extract_dataset_archive()\n",
    "    convert_train_set_to_images()\n",
    "\n",
    "\n",
    "def convert_videos_to_json(path_in, path_out):\n",
    "    os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml '\n",
    "              '--checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ' + path_in + ' --outdir ' + path_out + ' --sp')\n",
    "\n",
    "\n",
    "#prepare_dataset()\n",
    "#os.chdir(\"AlphaPose\")\n",
    "\n",
    "# for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/Violence/V_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/V_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "#for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/NonViolence/NV_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/NV_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "\n",
    "\n",
    "person_count = 5\n",
    "max_frames = 280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(5, 68))) # 150, 2048 = n_chunks, chunk_size\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "csv_dir = \"dataset/csvs\"\n",
    "\n",
    "csv_files = os.listdir(csv_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(csv_files, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11111435, 0.0]\n",
      "[0.11111155, 0.007142857]\n",
      "[0.11111136, 0.88928574]\n",
      "[0.11111177, 0.9]\n",
      "[0.11111191, 0.91071427]\n",
      "[0.11111213, 0.91785717]\n",
      "[0.11111271, 0.825]\n",
      "[0.111112475, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for file_name in train_files:\n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    violent = csv.violent[0]\n",
    "    \n",
    "    # Resize all CSV files to same arbitrary limit\n",
    "    csv = csv.drop('violent',axis=1)\n",
    "    \n",
    "    labels = csv.columns.values\n",
    "    \n",
    "    \n",
    "    for label in labels[2:]:\n",
    "        minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "        csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "    \n",
    "#     column_trans = ColumnTransformer(\n",
    "#         [('scaler', MinMaxScaler(),list(range(2,70)))],\n",
    "#         remainder='passthrough') \n",
    "#     column_trans.fit_transform(csv)\n",
    "    \n",
    "    data = csv.to_numpy()\n",
    "\n",
    "    val_cumulative = {}\n",
    "    frame_ids = set()\n",
    "    for person in data:\n",
    "        frame_ids.add(person[0])\n",
    "        frame, person_id = person[:2]\n",
    "\n",
    "        vel_array = []\n",
    "        for vel in range(5, 70, 4):\n",
    "            vel_array.append(vel)\n",
    "\n",
    "        total_vel = sum(vel_array)\n",
    "\n",
    "        if person_id in val_cumulative:\n",
    "            val_cumulative[person_id] += total_vel\n",
    "        else:\n",
    "            val_cumulative[person_id] = 0\n",
    "        \n",
    "        \n",
    "    val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "    top_people_dataset = {}\n",
    "    \n",
    "    current_frame = 0\n",
    "    for frame_id in frame_ids:\n",
    "        # Trim to 280 if above\n",
    "        if current_frame == max_frames:\n",
    "            break\n",
    "        top_people_dataset[frame_id] = []\n",
    "        people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "        fill_amount = person_count\n",
    "        for person in people:\n",
    "            top_people_dataset[frame_id].append(person[2:])\n",
    "            fill_amount -= 1\n",
    "        for i in range(0, fill_amount):\n",
    "            top_people_dataset[frame_id].append(np.zeros(68))\n",
    "        current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "    x_train = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "    if violent == 0:\n",
    "        y_train = np.zeros(len(x_train))\n",
    "    else:\n",
    "        y_train = np.ones(len(x_train))\n",
    "    \n",
    "#     x_train = np.array(csv.drop(['frame', 'object_id', 'violent'],axis=1))\n",
    "    print(model.train_on_batch(x_train, y_train))\n",
    "#     pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.111112155, 0.05]\n",
      "[0.11111246, 0.0]\n",
      "[0.11111248, 0.0]\n",
      "[0.1111123, 0.0]\n",
      "[0.11111256, 0.0]\n",
      "[0.1111124, 0.0]\n",
      "[0.111112714, 0.0035714286]\n",
      "[0.11111256, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for file_name in train_files:\n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    violent = csv.violent[0]\n",
    "    \n",
    "    # Resize all CSV files to same arbitrary limit\n",
    "    csv = csv.drop('violent',axis=1)\n",
    "    \n",
    "    labels = csv.columns.values\n",
    "    \n",
    "    \n",
    "    for label in labels[2:]:\n",
    "        minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "        csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "    \n",
    "#     column_trans = ColumnTransformer(\n",
    "#         [('scaler', MinMaxScaler(),list(range(2,70)))],\n",
    "#         remainder='passthrough') \n",
    "#     column_trans.fit_transform(csv)\n",
    "    \n",
    "    data = csv.to_numpy()\n",
    "\n",
    "    val_cumulative = {}\n",
    "    frame_ids = set()\n",
    "    for person in data:\n",
    "        frame_ids.add(person[0])\n",
    "        frame, person_id = person[:2]\n",
    "\n",
    "        vel_array = []\n",
    "        for vel in range(5, 70, 4):\n",
    "            vel_array.append(vel)\n",
    "\n",
    "        total_vel = sum(vel_array)\n",
    "\n",
    "        if person_id in val_cumulative:\n",
    "            val_cumulative[person_id] += total_vel\n",
    "        else:\n",
    "            val_cumulative[person_id] = 0\n",
    "        \n",
    "        \n",
    "    val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "    top_people_dataset = {}\n",
    "    \n",
    "    current_frame = 0\n",
    "    for frame_id in frame_ids:\n",
    "        # Trim to 280 if above\n",
    "        if current_frame == max_frames:\n",
    "            break\n",
    "        top_people_dataset[frame_id] = []\n",
    "        people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "        fill_amount = person_count\n",
    "        for person in people:\n",
    "            top_people_dataset[frame_id].append(person[2:])\n",
    "            fill_amount -= 1\n",
    "        for i in range(0, fill_amount):\n",
    "            top_people_dataset[frame_id].append(np.zeros(68))\n",
    "        current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "    x_test = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "    if violent == 0:\n",
    "        y_test= np.zeros(len(x_test))\n",
    "    else:\n",
    "        y_test = np.ones(len(x_test))\n",
    "\n",
    "\n",
    "    print(model.test_on_batch(x_test, y_test))\n",
    "# model.fit(x_train, y_train, epochs, batch_size=batchs, verbose=1)\n",
    "# model.save(\"rnn.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 5, 68)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
