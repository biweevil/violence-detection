{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from extract import extract_dataset_archive\n",
    "from split import convert_train_set_to_images\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Flatten, TimeDistributed, ConvLSTM2D, Conv3D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# from tensorflow.keras.layers.convolutional import \n",
    "# from tensorflow.keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "# os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml --checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ../dataset/NonViolence/NV_1.mp4 --outdir ../dataset --sp')\n",
    "\n",
    "def prepare_dataset():\n",
    "    extract_dataset_archive()\n",
    "    convert_train_set_to_images()\n",
    "\n",
    "\n",
    "def convert_videos_to_json(path_in, path_out):\n",
    "    os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml '\n",
    "              '--checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ' + path_in + ' --outdir ' + path_out + ' --sp')\n",
    "\n",
    "\n",
    "#prepare_dataset()\n",
    "#os.chdir(\"AlphaPose\")\n",
    "\n",
    "# for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/Violence/V_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/V_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "#for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/NonViolence/NV_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/NV_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "\n",
    "\n",
    "person_count = 5\n",
    "max_frames = 280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_8 (TimeDist multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               multiple                  67600     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             multiple                  101       \n",
      "=================================================================\n",
      "Total params: 67,701\n",
      "Trainable params: 67,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Flatten(input_shape=(5, 68))))\n",
    "model.add(LSTM(100))\n",
    "# model.add(Dense(1024))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(50))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "csv_dir = \"dataset/csvs\"\n",
    "\n",
    "csv_files = os.listdir(csv_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(csv_files, train_size=0.8)\n",
    "\n",
    "model.build(input_shape=(280, 5, 68))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22400 samples, validate on 5600 samples\n",
      "Epoch 1/10\n",
      "22400/22400 [==============================] - 5s 237us/sample - loss: 0.6433 - acc: 0.6014 - val_loss: 0.6700 - val_acc: 0.6068\n",
      "Epoch 2/10\n",
      "22400/22400 [==============================] - 4s 180us/sample - loss: 0.6142 - acc: 0.6224 - val_loss: 0.6603 - val_acc: 0.6175\n",
      "Epoch 3/10\n",
      "22400/22400 [==============================] - 4s 171us/sample - loss: 0.5941 - acc: 0.6385 - val_loss: 0.7261 - val_acc: 0.5870\n",
      "Epoch 4/10\n",
      "22400/22400 [==============================] - 4s 183us/sample - loss: 0.5762 - acc: 0.6514 - val_loss: 0.7150 - val_acc: 0.5991\n",
      "Epoch 5/10\n",
      "22400/22400 [==============================] - 4s 158us/sample - loss: 0.5610 - acc: 0.6577 - val_loss: 0.7263 - val_acc: 0.6037- loss: 0.5668\n",
      "Epoch 6/10\n",
      "22400/22400 [==============================] - 4s 167us/sample - loss: 0.5429 - acc: 0.6689 - val_loss: 0.7326 - val_acc: 0.6229\n",
      "Epoch 7/10\n",
      "22400/22400 [==============================] - 4s 180us/sample - loss: 0.5327 - acc: 0.6748 - val_loss: 0.7238 - val_acc: 0.6079\n",
      "Epoch 8/10\n",
      "22400/22400 [==============================] - 4s 183us/sample - loss: 0.5212 - acc: 0.6821 - val_loss: 0.7578 - val_acc: 0.5955\n",
      "Epoch 9/10\n",
      "22400/22400 [==============================] - 4s 178us/sample - loss: 0.5127 - acc: 0.6843 - val_loss: 0.8101 - val_acc: 0.6016\n",
      "Epoch 10/10\n",
      "22400/22400 [==============================] - 4s 171us/sample - loss: 0.5021 - acc: 0.6892 - val_loss: 0.8535 - val_acc: 0.5977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f8a9c5f388>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "trained = 0\n",
    "for file_name in train_files:\n",
    "    \n",
    "    if trained == 100:\n",
    "        break\n",
    "    trained += 1\n",
    "    \n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    current_frame = 0\n",
    "    \n",
    "    violent = 0\n",
    "    \n",
    "    if len(csv) > 0:\n",
    "        violent = csv.violent[0]\n",
    "\n",
    "        csv = csv.drop('violent',axis=1)\n",
    "\n",
    "        labels = csv.columns.values\n",
    "\n",
    "        for label in labels[2:]:\n",
    "            minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "            csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "\n",
    "        data = csv.to_numpy()\n",
    "\n",
    "        val_cumulative = {}\n",
    "        frame_ids = set()\n",
    "        for person in data:\n",
    "            frame_ids.add(person[0])\n",
    "            frame, person_id = person[:2]\n",
    "\n",
    "            vel_array = []\n",
    "            for vel in range(5, 70, 4):\n",
    "                vel_array.append(vel)\n",
    "\n",
    "            total_vel = sum(vel_array)\n",
    "\n",
    "            if person_id in val_cumulative:\n",
    "                val_cumulative[person_id] += total_vel\n",
    "            else:\n",
    "                val_cumulative[person_id] = 0\n",
    "\n",
    "\n",
    "        val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "        top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "        top_people_dataset = {}\n",
    "\n",
    "\n",
    "        for frame_id in frame_ids:\n",
    "            # Trim to 280 if above\n",
    "            if current_frame == max_frames:\n",
    "                break\n",
    "            top_people_dataset[frame_id] = []\n",
    "            people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "            fill_amount = person_count\n",
    "            for person in people:\n",
    "                top_people_dataset[frame_id].append(person[2:])\n",
    "                fill_amount -= 1\n",
    "            for i in range(0, fill_amount):\n",
    "                top_people_dataset[frame_id].append(np.zeros(68))\n",
    "            current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "    tmp = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "#     x_train.append(tmp)\n",
    "    for entry in tmp:\n",
    "        x_train.append(entry)\n",
    "\n",
    "#     x_train = np.append(x_train, np.asarray([list(top_people_dataset.values())]), axis=0)\n",
    "        \n",
    "#     print(x_train.shape)\n",
    "    \n",
    "    if violent == 0:\n",
    "        for entry in np.zeros(len(tmp)):\n",
    "            y_train.append(entry)\n",
    "    else:\n",
    "        for entry in np.ones(len(tmp)):\n",
    "            y_train.append(entry)\n",
    "#         y_train.append(np.ones(len(x_train)))\n",
    "    \n",
    "#     x_train = np.array(csv.drop(['frame', 'object_id', 'violent'],axis=1))\n",
    "# x_train = np.asarray(list(x_train))\n",
    "x_train = np.asarray(list(x_train))\n",
    "y_train = np.asarray(list(y_train))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11638993, 0.5285714]\n",
      "[0.115064405, 0.55714285]\n",
      "[0.11693399, 0.6785714]\n",
      "[0.117647484, 0.47142857]\n",
      "[0.115241244, 0.62857145]\n",
      "[0.115033664, 0.5535714]\n",
      "[0.1166664, 0.525]\n",
      "[0.115351476, 0.6464286]\n"
     ]
    }
   ],
   "source": [
    "for file_name in train_files:\n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    violent = csv.violent[0]\n",
    "    \n",
    "    # Resize all CSV files to same arbitrary limit\n",
    "    csv = csv.drop('violent',axis=1)\n",
    "    \n",
    "    labels = csv.columns.values\n",
    "    \n",
    "    \n",
    "    for label in labels[2:]:\n",
    "        minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "        csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "    \n",
    "#     column_trans = ColumnTransformer(\n",
    "#         [('scaler', MinMaxScaler(),list(range(2,70)))],\n",
    "#         remainder='passthrough') \n",
    "#     column_trans.fit_transform(csv)\n",
    "    \n",
    "    data = csv.to_numpy()\n",
    "\n",
    "    val_cumulative = {}\n",
    "    frame_ids = set()\n",
    "    for person in data:\n",
    "        frame_ids.add(person[0])\n",
    "        frame, person_id = person[:2]\n",
    "\n",
    "        vel_array = []\n",
    "        for vel in range(5, 70, 4):\n",
    "            vel_array.append(vel)\n",
    "\n",
    "        total_vel = sum(vel_array)\n",
    "\n",
    "        if person_id in val_cumulative:\n",
    "            val_cumulative[person_id] += total_vel\n",
    "        else:\n",
    "            val_cumulative[person_id] = 0\n",
    "        \n",
    "        \n",
    "    val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "    top_people_dataset = {}\n",
    "    \n",
    "    current_frame = 0\n",
    "    for frame_id in frame_ids:\n",
    "        # Trim to 280 if above\n",
    "        if current_frame == max_frames:\n",
    "            break\n",
    "        top_people_dataset[frame_id] = []\n",
    "        people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "        fill_amount = person_count\n",
    "        for person in people:\n",
    "            top_people_dataset[frame_id].append(person[2:])\n",
    "            fill_amount -= 1\n",
    "        for i in range(0, fill_amount):\n",
    "            top_people_dataset[frame_id].append(np.zeros(68))\n",
    "        current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "    x_test = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "    if violent == 0:\n",
    "        y_test= np.zeros(len(x_test))\n",
    "    else:\n",
    "        y_test = np.ones(len(x_test))\n",
    "\n",
    "\n",
    "    print(model.test_on_batch(x_test, y_test))\n",
    "# model.fit(x_train, y_train, epochs, batch_size=batchs, verbose=1)\n",
    "# model.save(\"rnn.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 5, 68)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
