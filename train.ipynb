{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from extract import extract_dataset_archive\n",
    "from split import convert_train_set_to_images\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Flatten, TimeDistributed, ConvLSTM2D, Conv3D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# from tensorflow.keras.layers.convolutional import \n",
    "# from tensorflow.keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "# os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml --checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ../dataset/NonViolence/NV_1.mp4 --outdir ../dataset --sp')\n",
    "\n",
    "def prepare_dataset():\n",
    "    extract_dataset_archive()\n",
    "    convert_train_set_to_images()\n",
    "\n",
    "\n",
    "def convert_videos_to_json(path_in, path_out):\n",
    "    os.system('python scripts/demo_inference.py --cfg pretrained_models/256x192_res152_lr1e-3_1x-duc.yaml '\n",
    "              '--checkpoint  pretrained_models/fast_421_res152_256x192.pth --video ' + path_in + ' --outdir ' + path_out + ' --sp')\n",
    "\n",
    "\n",
    "#prepare_dataset()\n",
    "#os.chdir(\"AlphaPose\")\n",
    "\n",
    "# for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/Violence/V_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/V_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "#for i in range(1, 1001):\n",
    "#    file_in = str(\"../dataset/NonViolence/NV_\") + str(i) + \".mp4\"\n",
    "#    file_out = str(\"../dataset/results/NV_\") + str(i) + \"_json\"\n",
    "#    convert_videos_to_json(file_in, file_out)\n",
    "\n",
    "\n",
    "\n",
    "person_count = 5\n",
    "max_frames = 280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  67600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  101       \n",
      "=================================================================\n",
      "Total params: 67,701\n",
      "Trainable params: 67,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Flatten(input_shape=(5, 68))))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.build(input_shape=(280, 5, 68))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = \"dataset/csvs\"\n",
    "\n",
    "csv_files = os.listdir(csv_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(csv_files, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "trained = 0\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "tested = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NV_477_json.csv\n",
      "(280, 5, 68)\n",
      "V_421_json.csv\n",
      "(280, 5, 68)\n",
      "V_967_json.csv\n",
      "(280, 5, 68)\n",
      "NV_99_json.csv\n",
      "(280, 5, 68)\n",
      "NV_800_json.csv\n",
      "(280, 5, 68)\n",
      "V_378_json.csv\n",
      "(280, 5, 68)\n",
      "V_222_json.csv\n",
      "(280, 5, 68)\n",
      "NV_306_json.csv\n",
      "(280, 5, 68)\n",
      "NV_760_json.csv\n",
      "(280, 5, 68)\n",
      "NV_172_json.csv\n",
      "(280, 5, 68)\n",
      "V_743_json.csv\n",
      "(280, 5, 68)\n",
      "V_157_json.csv\n",
      "(280, 5, 68)\n",
      "NV_834_json.csv\n",
      "(280, 5, 68)\n",
      "NV_687_json.csv\n",
      "(280, 5, 68)\n",
      "V_931_json.csv\n",
      "(280, 5, 68)\n",
      "V_528_json.csv\n",
      "(280, 5, 68)\n",
      "V_448_json.csv\n",
      "(280, 5, 68)\n",
      "NV_211_json.csv\n",
      "(280, 5, 68)\n",
      "V_89_json.csv\n",
      "(280, 5, 68)\n",
      "V_547_json.csv\n",
      "(280, 5, 68)\n",
      "NV_743_json.csv\n",
      "(280, 5, 68)\n",
      "NV_722_json.csv\n",
      "(280, 5, 68)\n",
      "V_583_json.csv\n",
      "(280, 5, 68)\n",
      "V_450_json.csv\n",
      "(280, 5, 68)\n",
      "V_264_json.csv\n",
      "(280, 5, 68)\n",
      "V_472_json.csv\n",
      "(280, 5, 68)\n",
      "V_870_json.csv\n",
      "(280, 5, 68)\n",
      "NV_156_json.csv\n",
      "(280, 5, 68)\n",
      "NV_177_json.csv\n",
      "(280, 5, 68)\n",
      "NV_17_json.csv\n",
      "(280, 5, 68)\n",
      "NV_840_json.csv\n",
      "(280, 5, 68)\n",
      "NV_731_json.csv\n",
      "(280, 5, 68)\n",
      "NV_383_json.csv\n",
      "(280, 5, 68)\n",
      "NV_129_json.csv\n",
      "(280, 5, 68)\n",
      "V_148_json.csv\n",
      "(280, 5, 68)\n",
      "V_535_json.csv\n",
      "(280, 5, 68)\n",
      "V_575_json.csv\n",
      "(280, 5, 68)\n",
      "NV_846_json.csv\n",
      "(280, 5, 68)\n",
      "NV_259_json.csv\n",
      "(280, 5, 68)\n",
      "V_281_json.csv\n",
      "(280, 5, 68)\n",
      "NV_715_json.csv\n",
      "(280, 5, 68)\n",
      "V_129_json.csv\n",
      "(280, 5, 68)\n",
      "NV_987_json.csv\n",
      "(280, 5, 68)\n",
      "V_350_json.csv\n",
      "(280, 5, 68)\n",
      "V_393_json.csv\n",
      "(280, 5, 68)\n",
      "NV_422_json.csv\n",
      "(280, 5, 68)\n",
      "V_231_json.csv\n",
      "(280, 5, 68)\n",
      "NV_827_json.csv\n",
      "(280, 5, 68)\n",
      "NV_403_json.csv\n",
      "(280, 5, 68)\n",
      "V_180_json.csv\n",
      "(280, 5, 68)\n",
      "NV_98_json.csv\n",
      "(280, 5, 68)\n",
      "NV_707_json.csv\n",
      "(280, 5, 68)\n",
      "V_871_json.csv\n",
      "(280, 5, 68)\n",
      "NV_686_json.csv\n",
      "(280, 5, 68)\n",
      "NV_258_json.csv\n",
      "(280, 5, 68)\n",
      "NV_833_json.csv\n",
      "(280, 5, 68)\n",
      "V_235_json.csv\n",
      "(280, 5, 68)\n",
      "V_103_json.csv\n",
      "(280, 5, 68)\n",
      "V_625_json.csv\n",
      "(280, 5, 68)\n",
      "V_586_json.csv\n",
      "(280, 5, 68)\n",
      "NV_973_json.csv\n",
      "(280, 5, 68)\n",
      "V_396_json.csv\n",
      "(280, 5, 68)\n",
      "NV_104_json.csv\n",
      "(280, 5, 68)\n",
      "NV_669_json.csv\n",
      "(280, 5, 68)\n",
      "V_174_json.csv\n",
      "(280, 5, 68)\n",
      "V_46_json.csv\n",
      "(280, 5, 68)\n",
      "V_456_json.csv\n",
      "(280, 5, 68)\n",
      "V_733_json.csv\n",
      "(280, 5, 68)\n",
      "V_109_json.csv\n",
      "(280, 5, 68)\n",
      "V_845_json.csv\n",
      "(280, 5, 68)\n",
      "NV_426_json.csv\n",
      "(280, 5, 68)\n",
      "NV_431_json.csv\n",
      "(280, 5, 68)\n",
      "NV_829_json.csv\n",
      "(280, 5, 68)\n",
      "V_685_json.csv\n",
      "(280, 5, 68)\n",
      "NV_937_json.csv\n",
      "(280, 5, 68)\n",
      "V_874_json.csv\n",
      "(280, 5, 68)\n",
      "NV_8_json.csv\n",
      "(280, 5, 68)\n",
      "V_389_json.csv\n",
      "(280, 5, 68)\n",
      "V_200_json.csv\n",
      "(280, 5, 68)\n",
      "V_886_json.csv\n",
      "(280, 5, 68)\n",
      "V_75_json.csv\n",
      "(280, 5, 68)\n",
      "V_86_json.csv\n",
      "(280, 5, 68)\n",
      "V_234_json.csv\n",
      "(280, 5, 68)\n",
      "NV_627_json.csv\n",
      "(280, 5, 68)\n",
      "V_100_json.csv\n",
      "(280, 5, 68)\n",
      "NV_89_json.csv\n",
      "(280, 5, 68)\n",
      "NV_185_json.csv\n",
      "(280, 5, 68)\n",
      "V_461_json.csv\n",
      "(280, 5, 68)\n",
      "V_432_json.csv\n",
      "(280, 5, 68)\n",
      "V_474_json.csv\n",
      "(280, 5, 68)\n",
      "NV_823_json.csv\n",
      "(280, 5, 68)\n",
      "V_150_json.csv\n",
      "(280, 5, 68)\n",
      "V_108_json.csv\n",
      "(280, 5, 68)\n",
      "NV_232_json.csv\n",
      "(280, 5, 68)\n",
      "NV_752_json.csv\n",
      "(280, 5, 68)\n",
      "NV_702_json.csv\n",
      "(280, 5, 68)\n",
      "NV_997_json.csv\n",
      "(280, 5, 68)\n",
      "NV_590_json.csv\n",
      "(280, 5, 68)\n",
      "V_53_json.csv\n",
      "(280, 5, 68)\n",
      "NV_682_json.csv\n",
      "(280, 5, 68)\n"
     ]
    }
   ],
   "source": [
    "for file_name in train_files:\n",
    "    \n",
    "    if trained == 100:\n",
    "        break\n",
    "    trained += 1\n",
    "    \n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    current_frame = 0\n",
    "    \n",
    "    violent = 0\n",
    "    print(file_name)\n",
    "    if len(csv) > 0:\n",
    "        violent = csv.violent[0]\n",
    "\n",
    "        csv = csv.drop('violent',axis=1)\n",
    "\n",
    "        labels = csv.columns.values\n",
    "\n",
    "        for label in labels[2:]:\n",
    "            minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "            csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "\n",
    "        data = csv.to_numpy()\n",
    "\n",
    "        val_cumulative = {}\n",
    "        frame_ids = set()\n",
    "        for person in data:\n",
    "            frame_ids.add(person[0])\n",
    "            frame, person_id = person[:2]\n",
    "\n",
    "            vel_array = []\n",
    "            for vel in range(5, 70, 4):\n",
    "                vel_array.append(vel)\n",
    "\n",
    "            total_vel = sum(vel_array)\n",
    "\n",
    "            if person_id in val_cumulative:\n",
    "                val_cumulative[person_id] += total_vel\n",
    "            else:\n",
    "                val_cumulative[person_id] = 0\n",
    "\n",
    "\n",
    "        val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "        top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "        top_people_dataset = {}\n",
    "\n",
    "        starter_map = {}\n",
    "        for key in top_people.keys():\n",
    "            starter_map[key] = np.zeros(68)\n",
    "\n",
    "        for frame_id in frame_ids:\n",
    "            # Trim to 280 if above\n",
    "            if current_frame == max_frames:\n",
    "                break\n",
    "            tmp_map = starter_map\n",
    "            people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "            for person in people:\n",
    "                tmp_map[person[1]] = person[2:]\n",
    "                \n",
    "            padded = list(tmp_map.values())\n",
    "                \n",
    "            for i in range(len(padded), person_count):\n",
    "                padded.append(np.zeros(68))\n",
    "\n",
    "            top_people_dataset[frame_id] = np.asarray(padded)\n",
    "            current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "    tmp = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "    print(tmp.shape)\n",
    "    \n",
    "    for entry in tmp:\n",
    "        x_train.append(entry)\n",
    "    \n",
    "    if violent == 0:\n",
    "        for entry in np.zeros(len(tmp)):\n",
    "            y_train.append(entry)\n",
    "    else:\n",
    "        for entry in np.ones(len(tmp)):\n",
    "            y_train.append(entry)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22400 samples, validate on 5600 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "22400/22400 [==============================] - 3s 133us/sample - loss: 0.6066 - acc: 0.6297 - val_loss: 0.6521 - val_acc: 0.6288\n",
      "Epoch 2/20\n",
      "22400/22400 [==============================] - 2s 94us/sample - loss: 0.5282 - acc: 0.6781 - val_loss: 0.7366 - val_acc: 0.6096\n",
      "Epoch 3/20\n",
      "22400/22400 [==============================] - 2s 88us/sample - loss: 0.4823 - acc: 0.6993 - val_loss: 0.7541 - val_acc: 0.6127\n",
      "Epoch 4/20\n",
      "22400/22400 [==============================] - 2s 90us/sample - loss: 0.4583 - acc: 0.7104 - val_loss: 0.7844 - val_acc: 0.6141\n",
      "Epoch 5/20\n",
      "22400/22400 [==============================] - 2s 97us/sample - loss: 0.4365 - acc: 0.7143 - val_loss: 0.7906 - val_acc: 0.6273\n",
      "Epoch 6/20\n",
      "22400/22400 [==============================] - 2s 96us/sample - loss: 0.4262 - acc: 0.7213 - val_loss: 0.7568 - val_acc: 0.6330\n",
      "Epoch 7/20\n",
      "22400/22400 [==============================] - 2s 89us/sample - loss: 0.4184 - acc: 0.7258 - val_loss: 0.9528 - val_acc: 0.6246\n",
      "Epoch 8/20\n",
      "22400/22400 [==============================] - 2s 90us/sample - loss: 0.4096 - acc: 0.7304 - val_loss: 0.8606 - val_acc: 0.6273\n",
      "Epoch 9/20\n",
      "22400/22400 [==============================] - 2s 97us/sample - loss: 0.4006 - acc: 0.7336 - val_loss: 0.8229 - val_acc: 0.6246\n",
      "Epoch 10/20\n",
      "22400/22400 [==============================] - 2s 88us/sample - loss: 0.3978 - acc: 0.7303 - val_loss: 1.0493 - val_acc: 0.6011\n",
      "Epoch 11/20\n",
      "22400/22400 [==============================] - 2s 90us/sample - loss: 0.3961 - acc: 0.7340 - val_loss: 0.9776 - val_acc: 0.6196\n",
      "Epoch 12/20\n",
      "22400/22400 [==============================] - 2s 94us/sample - loss: 0.3885 - acc: 0.7352 - val_loss: 0.9954 - val_acc: 0.6205\n",
      "Epoch 13/20\n",
      "22400/22400 [==============================] - 2s 91us/sample - loss: 0.3862 - acc: 0.7360 - val_loss: 0.9664 - val_acc: 0.6061\n",
      "Epoch 14/20\n",
      "22400/22400 [==============================] - 2s 90us/sample - loss: 0.3854 - acc: 0.7372 - val_loss: 1.1469 - val_acc: 0.6146\n",
      "Epoch 15/20\n",
      "22400/22400 [==============================] - 2s 96us/sample - loss: 0.3833 - acc: 0.7367 - val_loss: 1.0522 - val_acc: 0.6157\n",
      "Epoch 16/20\n",
      "22400/22400 [==============================] - 2s 94us/sample - loss: 0.3839 - acc: 0.7386 - val_loss: 1.0697 - val_acc: 0.6055\n",
      "Epoch 17/20\n",
      "22400/22400 [==============================] - 2s 88us/sample - loss: 0.3809 - acc: 0.7351 - val_loss: 1.1634 - val_acc: 0.6125\n",
      "Epoch 18/20\n",
      "22400/22400 [==============================] - 2s 92us/sample - loss: 0.3785 - acc: 0.7385 - val_loss: 1.1010 - val_acc: 0.6120\n",
      "Epoch 19/20\n",
      "22400/22400 [==============================] - 2s 90us/sample - loss: 0.3800 - acc: 0.7360 - val_loss: 1.3267 - val_acc: 0.6114\n",
      "Epoch 20/20\n",
      "22400/22400 [==============================] - 2s 105us/sample - loss: 0.3787 - acc: 0.7408 - val_loss: 1.1974 - val_acc: 0.6102\n",
      "28000/28000 [==============================] - 2s 57us/sample - loss: 0.5417 - acc: 0.7161\n"
     ]
    }
   ],
   "source": [
    "x_train = np.asarray(list(x_train))\n",
    "y_train = np.asarray(list(y_train))\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=100, validation_split=0.2)\n",
    "scores = model.evaluate(x_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in test_files:\n",
    "    csv = pd.read_csv(csv_dir + \"/\" + file_name)\n",
    "    \n",
    "    if tested == 100:\n",
    "        break\n",
    "    tested += 1\n",
    "    \n",
    "    violent = csv.violent[0]\n",
    "    \n",
    "    # Resize all CSV files to same arbitrary limit\n",
    "    csv = csv.drop('violent',axis=1)\n",
    "    \n",
    "    labels = csv.columns.values\n",
    "    \n",
    "    \n",
    "    for label in labels[2:]:\n",
    "        minmax_scale = preprocessing.MinMaxScaler().fit(csv[[label]])\n",
    "        csv[[label]] = minmax_scale.transform(csv[[label]])\n",
    "    \n",
    "    data = csv.to_numpy()\n",
    "\n",
    "    val_cumulative = {}\n",
    "    frame_ids = set()\n",
    "    for person in data:\n",
    "        frame_ids.add(person[0])\n",
    "        frame, person_id = person[:2]\n",
    "\n",
    "        vel_array = []\n",
    "        for vel in range(5, 70, 4):\n",
    "            vel_array.append(vel)\n",
    "\n",
    "        total_vel = sum(vel_array)\n",
    "\n",
    "        if person_id in val_cumulative:\n",
    "            val_cumulative[person_id] += total_vel\n",
    "        else:\n",
    "            val_cumulative[person_id] = 0\n",
    "        \n",
    "        \n",
    "    val_cumulative = {k: v for k, v in sorted(val_cumulative.items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_people = dict(itertools.islice(val_cumulative.items(), person_count))\n",
    "    top_people_dataset = {}\n",
    "    \n",
    "    current_frame = 0\n",
    "    \n",
    "    starter_map = {}\n",
    "    for key in top_people.keys():\n",
    "        starter_map[key] = np.zeros(68)\n",
    "\n",
    "    for frame_id in frame_ids:\n",
    "        # Trim to 280 if above\n",
    "        if current_frame == max_frames:\n",
    "            break\n",
    "        tmp_map = starter_map\n",
    "        people = np.array([item for item in data if item[0] == frame_id and item[1] in top_people.keys()])\n",
    "        for person in people:\n",
    "            tmp_map[person[1]] = person[2:]\n",
    "                \n",
    "        padded = list(tmp_map.values())\n",
    "                \n",
    "        for i in range(len(padded), person_count):\n",
    "            padded.append(np.zeros(68))\n",
    "\n",
    "        top_people_dataset[frame_id] = np.asarray(padded)\n",
    "        current_frame += 1\n",
    "            \n",
    "    \n",
    "    # Pad to 280 frames if below\n",
    "    for i in range(current_frame+1, max_frames+1):\n",
    "        top_people_dataset[i] = []\n",
    "        for j in range(0, person_count):\n",
    "            top_people_dataset[i].append(np.zeros(68))\n",
    "                 \n",
    "                \n",
    "    tmp = np.asarray(list(top_people_dataset.values()))\n",
    "    \n",
    "    for entry in tmp:\n",
    "        x_test.append(entry)\n",
    "    \n",
    "    if violent == 0:\n",
    "        for entry in np.zeros(len(tmp)):\n",
    "            y_test.append(entry)\n",
    "    else:\n",
    "        for entry in np.ones(len(tmp)):\n",
    "            y_test.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 2s 64us/sample - loss: 1.1851 - acc: 0.5669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1850561106053166, 0.56685716]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.asarray(list(x_test))\n",
    "y_test = np.asarray(list(y_test))\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=100, validation_split=0.2)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
